{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME TO THE ML @ SAPIENZA HACKATHON 2023/24!\n",
    "\n",
    "This is a short welcome kit to familiarize with the topics of the Hackathon: *digital music* ðŸŽ¹\n",
    "\n",
    "Please read this notebook from start to end, because you'll need some of these functions and tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this version of the notebook is supposed to be ran on your local computer.\n",
    "\n",
    "If you want to run the tutorial on Google Colab, please use the other notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have followed all instructions in the readme in order to have a properly-set virtual environment to run your code on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals\n",
    "\n",
    "Let's first import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If you *don't* use Colab and want to use your laptop's GPU, make sure to install `torch` with CUDA enabled:\n",
    "\n",
    "```pip3 install torch torchaudio --index-url https://download.pytorch.org/whl/cu118 ```\n",
    "\n",
    "Visit [https://pytorch.org/](the official website) to get the exact string for your OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some utility functions that we will need in this demo. You don't need to understand how they work to complete this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start playing some sounds! ðŸŽ¶\n",
    "\n",
    "We use the `AudioSegment` class to load sounds and play them. Execute the following cell to get a small embedded player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav(\"./test.wav\")\n",
    "player(audio, zoom=1, title=\"test waveform\")\n",
    "\n",
    "# push the play button!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you enjoy those smooth notes?\n",
    "\n",
    "You might be wondering what is that function plot we see below the player. If you already know what it is, a brief refresher doesn't hurt. If it's the first time you delve into this, welcome to the world of **Digital Sound Processing**!! ðŸ•º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to represent sound is as a **list of samples**. Let's try to zoom into the plot above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player(audio, zoom=60., title=\"test waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zoom only shows a small portion of the entire soundwave. On the *x*-axis we see time; on the *y*-axis, we have the **sample** amplitude. Therefore, in the plot above we are looking at a window of roughly $3,500$ samples.\n",
    "\n",
    "How many samples in time are we using to represent the entire audio file? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(audio.get_array_of_samples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of samples for just $8$ seconds of music! If each of these samples was a letter of the alphabet, we would have a book of $150$ pages!\n",
    "\n",
    "That's because we are representing sound using $24,000$ samples per second; in signal processing we say that the **sample rate** (or **frame rate**) of this example is $24$ kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio.frame_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to process audio more quickly, or you need to save some memory, hence it can be useful to decrease the sample rate. Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_downsampled = audio.set_frame_rate(4000)\n",
    "player(audio_downsampled, zoom=1., title=\"downsampled waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sound quality has decreased, but it sounds acceptable given the 6x times reduction in memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, each of the $33,920$ samples of this downsampled soundwave is stored with a certain precision; is it just $1$ byte per sample? $4$ bytes? Are these integer values, or floating point numbers? Are these signed or unsigned numbers?\n",
    "\n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio.sample_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sample width** is the number of bytes used to represent each sample. As you can imagine, it determines the sound quality. The wider, the merrier!\n",
    "\n",
    "Let's try to reduce the sample width to just $1$ byte per sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lowres = audio.set_sample_width(1)\n",
    "player(audio_lowres, zoom=1., title=\"lowres waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still hear what's going on, but there's strong background noise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, there is a trade-off between sound quality and efficiency when we deal with digital audio; however, for a ML project, it might be good to reduce quality whenever needed, if it makes things easier to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms\n",
    "\n",
    "Representing sound as a scalar function over time is not the only possibility.\n",
    "\n",
    "In fact, it is very common in sound processing to use **spectrograms**. Let's compute one right away! First, we choose some parameters for the spectrogram computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"  # change to \"cpu\" if you don't have a GPU\n",
    "\n",
    "# NOTE: these parameters are optimized for a sample rate of 44.1 kHz\n",
    "params = SpectrogramParams(\n",
    "            sample_rate=44100,\n",
    "            stereo=False,\n",
    "            step_size_ms=10,\n",
    "            min_frequency=20,\n",
    "            max_frequency=20000,\n",
    "            num_frequencies=512,\n",
    "        )\n",
    "\n",
    "converter = SpectrogramConverter(params=params, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the computed spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio.set_frame_rate(params.sample_rate)\n",
    "spectrogram = converter.spectrogram_from_audio(audio)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(librosa.power_to_db(spectrogram.squeeze()), origin='lower', aspect='auto', interpolation=\"nearest\")\n",
    "plt.ylabel('Freq. bin')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.title('Original', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above, we observe a few things:\n",
    "\n",
    "- The *x*-axis represents **time**.\n",
    "- The *y*-axis seems to represent musical notes! Look at the yellow arrow-like traces: they go up and down just like the notes in the short music piece.\n",
    "- There is an empty region on the right, corresponding to *silence*.\n",
    "\n",
    "The reason why the $y$-axis seems to represent musical notes, it's because it represents **frequency**, i.e. the oscillation speed of the soundwave. And in fact, according to music theory, **different notes correspond to different frequencies**.\n",
    "\n",
    "Can we go back from spectrogram to soundwave? Sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recovered = converter.audio_from_spectrogram(spectrogram, do_apply_filters=False)\n",
    "player(audio_recovered, zoom=1., title=\"recovered waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectrogram transform is *invertible*. But it may be a better representation for ML pipelines, since essentially it is a 2D image!\n",
    "\n",
    "Also, it's easier to **manipulate**. For example, you can apply image processing techniques like smoothing, interpolation and deformation, then convert back to the time domain of the soundwave, and listen to what new sounds you created. It's actually pretty fun! ðŸ¤ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "In addition to the spectrogram, there are many other **audio features** you can extract! Things like the *spectral centroid*, or the *bandwidth*, or the *zero crossing rate* are informative and can help in ML tasks.\n",
    "\n",
    "The `librosa` library comes with ready-to-use functions to compute audio features. [Check the docs!](https://librosa.org/doc/0.10.2/feature.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing music\n",
    "\n",
    "Don't worry, you don't have to be a great composer -- you just have to be a good listener!ðŸŽ§\n",
    "\n",
    "Let's load a song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modsong import MODSong, Sample\n",
    "\n",
    "song = MODSong()\n",
    "song.load_from_file(\"./demo.mod\")\n",
    "\n",
    "player(song, zoom=1., title=\"demo song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The song is already composed for you, but you can change a few things!\n",
    "\n",
    "For example, here's a list of its instruments (here they are called *samples*, but don't confuse these samples with the soundwave samples we explained above!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(song.samples)):\n",
    "    print(f\"Sample {i}: {song.samples[i].name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the song has $31$ samples in total, but not all of them are used! The used ones have a name. For example, sample n. $6$ is called \"polysynth\", which is some kind of synthetic sound.\n",
    "\n",
    "Let's have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6 = AudioSegment(data=song.samples[6].waveform, sample_width=2, frame_rate=44100, channels=1)\n",
    "player(s6, zoom=1., title=f\"sample 6: {song.samples[6].name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the bassdrum, corresponding to sample $14$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6 = AudioSegment(data=song.samples[14].waveform, sample_width=2, frame_rate=44100, channels=1)\n",
    "player(s6, zoom=1., title=f\"sample 14: {song.samples[14].name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we modify the song, so that it only plays the bassdrum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.keep_sample(14)\n",
    "player(song, zoom=1., title=\"demo song with bassdrum only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the song object is *stateful*! We can't recover the samples we removed. So we need to reload the song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = MODSong()\n",
    "song.load_from_file(\"./demo.mod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove all the drums, and play the song again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.remove_sample(12)\n",
    "song.remove_sample(13)\n",
    "song.remove_sample(14)\n",
    "\n",
    "player(song, zoom=1., title=\"demo song without drums\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it would be cool to load our own instruments into the song!\n",
    "\n",
    "Let's load a \"clap\" sound, and put it where the bassdrum was before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clap = AudioSegment.from_wav(\"./clap.wav\")\n",
    "player(clap, zoom=1, title=\"clap waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds a bit harsh, right? That's because we need to play it at a higher note! Let's put it inside the song to see how it sounds, and then we'll change its pitch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.samples[14] = Sample()  # replace the bassdrum with a new sample\n",
    "song.samples[14].name = \"clap\"\n",
    "song.samples[14].waveform = clap.get_array_of_samples()\n",
    "\n",
    "player(song, zoom=1., title=\"demo song with claps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean it up ðŸ§¹ We are going to turn down its volume a bit, and raise its pitch by $12$ semitones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.samples[14].volume = 40  # the maximum volume is 64!\n",
    "song.tune_sample(sample_idx=14, semitone=12)  # raise the pitch by 12 semitones\n",
    "\n",
    "player(song, zoom=1., title=\"demo song with claps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also play with pitches to modify the main melody.\n",
    "\n",
    "For example, below we *lower* the pitch of the lead melody by $3$ semitones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.tune_sample(sample_idx=4, semitone=-3)  # lower the pitch by 3 semitones\n",
    "\n",
    "player(song, zoom=1., title=\"demo song out of tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect is that the main melody is now out of tune with the rest of the song ðŸŽ¶ðŸ‘Ž\n",
    "\n",
    "It seems useless here, but being able to finetune the pitch of individual instruments will be important for the main challenge of this hackathon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
